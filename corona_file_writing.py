# -*- coding: utf-8 -*-
"""Corona_File_Writing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFAJ64kMgwWNcgwMqc2NbEZDkyqbJ4Oi
"""

from google.colab import drive
drive.mount('/content/gdrive')

from pickle import load,dump
from sklearn.feature_extraction.text import TfidfVectorizer
from pickle import load,dump  
import pandas as pd
import numpy as np
from copy import deepcopy

####TD-IDF and File TextFile 


#Modified cleanup
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.stem.snowball import SnowballStemmer

from copy import deepcopy
import re

import string


l_sw=stopwords.words("english")
tk=TweetTokenizer()
snow=SnowballStemmer("english")


def clean_without(st):

  #Lowercase
  temp=deepcopy(st)
  temp=temp.lower()

  #Remove whitespace
  temp=temp.replace("\n"," ").replace("\r"," ")

  #Remove URL
  temp = re.sub(r"http\S+", "", temp)

  #Tokenise
  tokens=tk.tokenize(temp)

  #remove punctuations, hashtags and stopwords
  cl_1=[]
  for i in tokens:
    if i not in string.punctuation and i not in l_sw and i!="...":
      if i[0]=='#':
        cl_1.append(i[1:])
      else:
        cl_1.append(i)
  
  #Remove emojis and serialise
  final=''

  ff=open("/content/gdrive/My Drive/Colab Notebooks/Corona_Clusters/single/to_be_deleted1.txt","w")

  for i in cl_1:
    try:
      ff.write(i)
      final+=(i+" ")
    except:
      pass
  
  ff.close()

  return final[:-1]


def file_write(cluster_labels,l_id,nm):
  temp={}
  
  """
  for i in range(len(cluster_labels)):
    if int(cluster_labels[i]) not in temp:
      temp[int(cluster_labels[i])]=[l_id[i]]
    else:
      temp[int(cluster_labels[i])].append(l_id[i])

  """
    
  #nnnfff=open("/content/gdrive/My Drive/Colab Notebooks/Corona_Clusters/text/average/"+nm+"average_tp.txt","w",encoding="utf-8")
  nnnfff=open("/content/gdrive/My Drive/Colab Notebooks/Corona_Clusters/news/"+nm+"_tp.txt","w",encoding="utf-8")
  print("Inside Here")

  temp=[0]
  for i in temp:
    qq=nnnfff.write("\n\n\nTop Words "+str(i)+"\n")
    #tw=[unpack[k]["tweet_text"] for k in temp[i]]
    tw=[unpack[k]["tweet_text"] for k in l_id]
    nnnfff.write(str(len(tw))+"\n\n")


    try:
      vec=TfidfVectorizer(ngram_range=(2,4),min_df=5,max_features=10,preprocessor=clean_without)
      vec_f=vec.fit_transform(tw).toarray()
    except:
      vec=TfidfVectorizer(ngram_range=(2,4),max_features=10,preprocessor=clean_without)
      vec_f=vec.fit_transform(tw).toarray()

    fn_n = np.array(vec.get_feature_names())

    for j in fn_n:
      nnnfff.write(j+"\t")
    nnnfff.write("\n\n")

    #for j in temp[i]:
    for j in l_id:
      nnnfff.write(unpack[j]["tweet_text"].replace("\n"," ").replace("\r"," ")+"\n")

  nnnfff.close()

nnff=open("/content/gdrive/My Drive/Colab Notebooks/corona_news_unpacked.pickle","rb")
unpack=load(nnff)
nnff.close()

nnff=open("/content/gdrive/My Drive/Colab Notebooks/corona_news_10mins.pickle","rb")
consol=load(nnff)
nnff.close()

consol.keys()

for i in [0]:
  for j in [0]:
    nm=str(i)+"-"+str(j)
    #nnff=open("/content/gdrive/My Drive/Colab Notebooks/Corona_Clusters/average/"+nm+"_average.pickle","rb")
    #a=load(nnff)
    l_id=[k["tweet_id"] for k in consol[i][j]]
    #file_write(a[1],a[2],nm)
    file_write("test",l_id,nm)
    #print(a[0])