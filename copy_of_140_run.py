# -*- coding: utf-8 -*-
"""Copy of 140_Run

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pbSStJ_ctN9hZtZNVw_7Sp25DhvGRI1E
"""

from google.colab import drive
drive.mount('/content/gdrive')

f=open("/content/gdrive/My Drive/Colab Notebooks/corona_tweets_unpacked.pickle","rb")

from pickle import load
a=load(f)
f.close()

type(a)

import requests
import json

res=requests.post("http://www.sentiment140.com/api/bulkClassifyJson",json.dumps({"data":[{"text":a["tweet_text"],"tweet_id":a["tweet_id"]}]}))

l=json.loads(res.content)
l

def srt(dc,field):
    global f_dict

    for i in dc:

        if i['polarity']==0:
            val=-1
        elif i['polarity']==2:
            val=0
        elif i['polarity']==4:
            val=1
        else:
            print("Error")

        
        if i["tweet_id"] not in f_dict:
            f_dict[i["tweet_id"]]={field:val}
        else:
            f_dict[i["tweet_id"]][field]=val
    
def serl(lt):
    
    temp=''
    for i in lt:
        temp+=(i+" ")
    return temp[:-1]

f_dict={}

srt(l["data"],"tweet_text")

import time

agg_list=[]

for i in a:
  agg_list.append(a[i])

agg_list[1]

"""Make 3 different sets (Tweet_Text, Snowball, Porter). Then consolidate and save the dict into pickle Post_140"""

f_dict={}
temp_dict={}
from copy import deepcopy


flag=0

count,over_ct=0,0
ct=0

while 1:
    temp=[]
    for i in range(10000):
        try:
            a=deepcopy(agg_list[ct])
            temp.append({"text":a["tweet_text"],"tweet_id":a["tweet_id"]})
            count+=1
            ct+=1

        except:
            flag=1
            break

    res=requests.post("http://www.sentiment140.com/api/bulkClassifyJson",json.dumps({"data":temp}))

    if res.status_code==200:
        temp=json.loads(res.content)
        srt(temp["data"],"tweet_text")
    else:
        print("Some Prob")
        break
    
    if count>100000:
        over_ct+=1
        print(over_ct)
        count=0
    if flag:
        break
print("Done")

f_dict[1252966551529172993]

from pickle import dump
f1=open("/content/gdrive/My Drive/Colab Notebooks/Corona_Post_140.pickle","wb")
dump(f_dict,f1)
f1.close()

#Inits.
f_dict={}
temp_dict={}

f=open("/content/gdrive/My Drive/Colab Notebooks/VADER_File.pickle","rb")
flag=0

count,over_ct=0,0

start_time=time.time()
#Tweet_Text
while 1:
    temp=[]
    for i in range(10000):
        try:
            a=load(f)
            temp.append({"text":a["tweet_text"],"tweet_id":a["tweet_id"]})
            count+=1

        except:
            flag=1
            break

    res=requests.post("http://www.sentiment140.com/api/bulkClassifyJson",json.dumps({"data":temp}))

    if res.status_code==200:
        temp=json.loads(res.content)
        srt(temp["data"],"tweet_text")
    else:
        print("Some Prob")
        break
    
    if count>100000:
        over_ct+=1
        print(over_ct)
        count=0
    if flag:
        break
print("Done")

end_time=time.time()

print(end_time-start_time)

len(f_dict)
c=0
for i in f_dict:
    print(f_dict[i])
    c+=1
    if c==5:
        break

temp_dict={}
f.close()

f=open("/content/gdrive/My Drive/Colab Notebooks/VADER_File.pickle","rb")
flag=0
count,over_ct=0,0

#snowball
start_time=time.time()
while 1:
    temp=[]
    for i in range(10000):
        try:
            a=load(f)
            temp.append({"text":serl(a["snowball"]),"tweet_id":a["tweet_id"]})
            count+=1

        except:
            flag=1
            break

    res=requests.post("http://www.sentiment140.com/api/bulkClassifyJson",json.dumps({"data":temp}))

    if res.status_code==200:
        temp=json.loads(res.content)
        srt(temp["data"],"snowball")
    else:
        print("Some Prob")
        break
    
    if count>100000:
        over_ct+=1
        print(over_ct)
        count=0
    if flag:
        break

end_time=time.time()
print(end_time-start_time)
print("Done")

print(len(f_dict))
c=0
for i in f_dict:
    print(f_dict[i])
    c+=1
    if c==5:
        break

temp_dict={}
f.close()

f=open("/content/gdrive/My Drive/Colab Notebooks/VADER_File.pickle","rb")
flag=0
count,over_ct=0,0
#porter
start_time=time.time()
while 1:
    temp=[]
    for i in range(10000):
        try:
            a=load(f)
            temp.append({"text":serl(a["porter"]),"tweet_id":a["tweet_id"]})
            count+=1

        except:
            flag=1
            break

    res=requests.post("http://www.sentiment140.com/api/bulkClassifyJson",json.dumps({"data":temp}))

    if res.status_code==200:
        temp=json.loads(res.content)
        srt(temp["data"],"porter")
    else:
        print("Some Prob")
        break
    
    if count>100000:
        over_ct+=1
        print(over_ct)
        count=0
    if flag:
        break
end_time=time.time()
print(end_time-start_time)
print("Done")

print(len(f_dict))
c=0
for i in f_dict:
    print(f_dict[i])
    c+=1
    if c==5:
        break

nff=open("/content/gdrive/My Drive/Colab Notebooks/Post_140.pickle","wb")

from pickle import dump

dump(f_dict,nff)

nff.close()

