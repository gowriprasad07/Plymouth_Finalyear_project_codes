# -*- coding: utf-8 -*-
"""Classifier_Trials_V2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QB43HammSDQvXS7xk3dSdg2Ol3J5Yqmx
"""

from google.colab import drive
drive.mount('/content/gdrive')

#Include Function for Stopword Removal
from nltk.corpus import stopwords
import nltk
nltk.download("stopwords")
l_sw=stopwords.words("english")

def stopw(s):
  temp=''
  s=s.split()
  for i in s:
    if i.lower() not in l_sw:
      temp+=(i+" ")
  
  return temp[:-1]

def serial(s):
  temp=''
  for i in s:
    temp+=(i+" ")
  
  return temp[:-1]

from pickle import load,dump
f=open("/content/gdrive/My Drive/Colab Notebooks/Updated_Dataset_CSA/Dataset_V2_Labelled_Corrected.pickle","rb")

l=load(f)
len(l)

for i in l:
  print(i)
  break

l[243566991414284288]

#If it needs raw training

l_x,l_y=[],[]

for i in l:
  l_x.append(stopw(l[i]["tweet_text"]))
  l_y.append(l[i]["label"])

# If train set needs to contain stems
"""
for i in l:
  l_x.append(serial(l[i]["snow"]))
  l_y.append(l[i]["label"])
"""

len(l_x)

from random import shuffle
from sklearn.model_selection import train_test_split
import pandas as pd


#train_set=l[:int(0.6*len(l))]
#test_set=l[int(0.6*len(l)):]

x_train,x_test,y_train,y_test=train_test_split(l_x,l_y,train_size=0.75,stratify=l_y)

len(x_test)

print(x_train[0])
print(y_train[0])

direct="Custom_V5/V1_1"

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from joblib import dump, load # used for saving and loading sklearn objects
from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices

#Unigram - Count

unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))
unigram_vectorizer.fit(x_train)

dump(unigram_vectorizer, '/content/gdrive/My Drive/Colab Notebooks/'+direct+'/unigram_vectorizer.joblib')
X_train_unigram = unigram_vectorizer.transform(x_train)
save_npz('/content/gdrive/My Drive/Colab Notebooks/'+direct+'/X_train_unigram.npz', X_train_unigram)



#Unigram - TF IDF

unigram_tf_idf_transformer = TfidfTransformer()
unigram_tf_idf_transformer.fit(X_train_unigram)

dump(unigram_tf_idf_transformer, '/content/gdrive/My Drive/Colab Notebooks/'+direct+'/unigram_tf_idf_transformer.joblib')
X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)
save_npz('/content/gdrive/My Drive/Colab Notebooks/'+direct+'/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)

#Bigram - Count

bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))
bigram_vectorizer.fit(x_train)

dump(bigram_vectorizer, '/content/gdrive/My Drive/Colab Notebooks/'+direct+'/bigram_vectorizer.joblib')
X_train_bigram = bigram_vectorizer.transform(x_train)
save_npz('/content/gdrive/My Drive/Colab Notebooks/'+direct+'/X_train_bigram.npz', X_train_bigram)


#Bigram - TF IDF
bigram_tf_idf_transformer = TfidfTransformer()
bigram_tf_idf_transformer.fit(X_train_bigram)

dump(bigram_tf_idf_transformer, '/content/gdrive/My Drive/Colab Notebooks/'+direct+'/bigram_tf_idf_transformer.joblib')
X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)
save_npz('/content/gdrive/My Drive/Colab Notebooks/'+direct+'/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix

def tss(X,y,title):
  training_x,valid_x,training_y,valid_y=train_test_split(X,y,train_size=0.75,stratify=y)
  
  clf = SGDClassifier(shuffle=True)
  clf.fit(training_x,training_y)

  train_score = clf.score(training_x,training_y)
  valid_score = clf.score(valid_x,valid_y)

  print(title,"\nTrain Score :",round(train_score,2),"\nValidation Score :",round(valid_score,2),"\n")

tss(X_train_unigram, y_train, 'Unigram Counts') 
tss(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')
tss(X_train_bigram, y_train, 'Bigram Counts')
tss(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

#Replace here!
training_x = X_train_bigram

#Phase 1 

clf = SGDClassifier(shuffle=True)
distributions = dict(
    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
    learning_rate=['optimal', 'invscaling', 'adaptive'],
    eta0=uniform(loc=1e-7, scale=1e-2)
)

random_search_cv = RandomizedSearchCV(
    estimator=clf,
    param_distributions=distributions,
    cv=5,
    n_iter=100
)

random_search_cv.fit(training_x, y_train)

print("Best Params :", random_search_cv.best_params_)
print("Best Scored :", random_search_cv.best_score_)

#Phase 2:

clf = SGDClassifier(shuffle=True)

distributions = dict(
    penalty=['l1', 'l2', 'elasticnet'],
    alpha=uniform(loc=1e-6, scale=1e-4)
)

random_search_cv = RandomizedSearchCV(
    estimator=clf,
    param_distributions=distributions,
    cv=5,
    n_iter=100
)

random_search_cv.fit(training_x, y_train)
print('Best params :',random_search_cv.best_params_)
print('Best score :',random_search_cv.best_score_)

sgd_classifier = random_search_cv.best_estimator_
dump(random_search_cv.best_estimator_, '/content/gdrive/My Drive/Colab Notebooks/'+direct+'/sgd_classifier_with_bgc.joblib')

#Replace Here!

testing_x=bigram_vectorizer.transform(x_test)
#testing_x = bigram_tf_idf_transformer.transform(testing_x)

score = sgd_classifier.score(testing_x, y_test)
print(score)



